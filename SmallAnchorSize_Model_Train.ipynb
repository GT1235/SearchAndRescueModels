{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "hc2gNDG23sG4",
        "lbcwj2tVeRZ3",
        "n9NtGZ8GeXpS",
        "TmsMHxeUjBrw",
        "QhP5xcSNjG91"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sx_7k3FdsyTH"
      },
      "source": [
        "## Download the data"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "8lW34c8sn4aa"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dff6yoBQCfDU"
      },
      "source": [
        "!wget https://michaeltpublic.s3.amazonaws.com/sarnet.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "IHqBPieFuzLV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gqXfd5I5Cnc_"
      },
      "source": [
        "!unzip -q -o sarnet.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_jJ-TAYQtOgt"
      },
      "source": [
        "## Install Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5qh697Suu4Gt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m pip install pyyaml==5.1\n",
        "import sys, os, distutils.core\n",
        "# Note: This is a faster way to install detectron2 in Colab, but it does not include all functionalities (e.g. compiled operators).\n",
        "# See https://detectron2.readthedocs.io/tutorials/install.html for full installation instructions\n",
        "!git clone 'https://github.com/facebookresearch/detectron2'\n",
        "dist = distutils.core.run_setup(\"./detectron2/setup.py\")\n",
        "!python -m pip install {' '.join([f\"'{x}'\" for x in dist.install_requires])}\n",
        "sys.path.insert(0, os.path.abspath('./detectron2'))\n"
      ],
      "metadata": {
        "id": "gvl9vzvBsNyK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nZk_Larh353j"
      },
      "source": [
        "# Setup Data Loaders"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UDMhazr0FxF-"
      },
      "source": [
        "# Some basic setup:\n",
        "# Setup detectron2 logger\n",
        "import detectron2\n",
        "from detectron2.utils.logger import setup_logger\n",
        "setup_logger()\n",
        "\n",
        "# import some common libraries\n",
        "import numpy as np\n",
        "import os, json, cv2, random\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "# import some common detectron2 utilities\n",
        "from detectron2 import model_zoo\n",
        "from detectron2.engine import DefaultPredictor\n",
        "from detectron2.config import get_cfg\n",
        "from detectron2.utils.visualizer import Visualizer\n",
        "from detectron2.data import MetadataCatalog, DatasetCatalog\n",
        "\n",
        "from detectron2 import model_zoo\n",
        "from detectron2.config import get_cfg\n",
        "from detectron2.utils.visualizer import Visualizer\n",
        "from detectron2.data import MetadataCatalog, DatasetCatalog\n",
        "\n",
        "from detectron2.engine import DefaultTrainer\n",
        "\n",
        "from detectron2.evaluation import COCOEvaluator, inference_on_dataset\n",
        "from detectron2.data import build_detection_test_loader\n",
        "\n",
        "from detectron2.data.datasets import register_coco_instances\n",
        "try:\n",
        "  register_coco_instances(\"dataset_train_1\", {}, \"/content/dataset/annotations/train.json\", \"/content/dataset/train\")\n",
        "  register_coco_instances(\"dataset_val_1\", {}, \"/content/dataset/annotations/val.json\", \"/content/dataset/val\")\n",
        "  register_coco_instances(\"dataset_test_1\", {}, \"/content/dataset/annotations/test.json\", \"/content/dataset/test\")\n",
        "except AssertionError:\n",
        "  print(\"INFO: Datasets already registered.\")\n",
        "\n",
        "para_metadata = MetadataCatalog.get(\"dataset_train_1\")\n",
        "dataset_dicts = DatasetCatalog.get(\"dataset_train_1\")\n",
        "\n",
        "para_metadata_val = MetadataCatalog.get(\"dataset_val_1\")\n",
        "dataset_dicts_val = DatasetCatalog.get(\"dataset_val_1\")\n",
        "\n",
        "para_metadata_test = MetadataCatalog.get(\"dataset_test_1\")\n",
        "dataset_dicts_test = DatasetCatalog.get(\"dataset_test_1\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "from detectron2.engine.hooks import HookBase\n",
        "from detectron2.evaluation import inference_context\n",
        "from detectron2.utils.logger import log_every_n_seconds\n",
        "from detectron2.data import DatasetMapper, build_detection_test_loader\n",
        "import detectron2.utils.comm as comm\n",
        "import torch\n",
        "import time\n",
        "import datetime\n",
        "import logging\n",
        "import numpy as np\n",
        "\n",
        "class LossEvalHook(HookBase):\n",
        "    def __init__(self, eval_period, model, data_loader):\n",
        "        self._model = model\n",
        "        self._period = eval_period\n",
        "        self._data_loader = data_loader\n",
        "\n",
        "    def _do_loss_eval(self):\n",
        "        # Copying inference_on_dataset from evaluator.py\n",
        "        total = len(self._data_loader)\n",
        "        num_warmup = min(5, total - 1)\n",
        "\n",
        "        start_time = time.perf_counter()\n",
        "        total_compute_time = 0\n",
        "        losses = []\n",
        "        for idx, inputs in enumerate(self._data_loader):\n",
        "            if np.random.rand() > 0.1:\n",
        "              continue\n",
        "            if torch.cuda.is_available():\n",
        "                torch.cuda.synchronize()\n",
        "            loss_batch = self._get_loss(inputs)\n",
        "\n",
        "            losses.append(loss_batch)\n",
        "        mean_loss = np.mean(losses)\n",
        "        print(\"validation loss on 10% of val data: {}\".format(mean_loss))\n",
        "        self.trainer.storage.put_scalar('validation_loss', mean_loss)\n",
        "        comm.synchronize()\n",
        "\n",
        "        return losses\n",
        "\n",
        "    def _get_loss(self, data):\n",
        "        # How loss is calculated on train_loop\n",
        "        metrics_dict = self._model(data)\n",
        "        metrics_dict = {\n",
        "            k: v.detach().cpu().item() if isinstance(v, torch.Tensor) else float(v)\n",
        "            for k, v in metrics_dict.items()\n",
        "        }\n",
        "        total_losses_reduced = sum(loss for loss in metrics_dict.values())\n",
        "        return total_losses_reduced\n",
        "\n",
        "\n",
        "    def after_step(self):\n",
        "        next_iter = self.trainer.iter + 1\n",
        "        is_final = next_iter == self.trainer.max_iter\n",
        "        if is_final or (self._period > 0 and next_iter % self._period == 0):\n",
        "            self._do_loss_eval()\n",
        "        self.trainer.storage.put_scalars(timetest=12)\n",
        "\n",
        "class MyTrainer(DefaultTrainer):\n",
        "    @classmethod\n",
        "    def build_evaluator(cls, cfg, dataset_name, output_folder=None):\n",
        "        if output_folder is None:\n",
        "            output_folder = os.path.join(cfg.OUTPUT_DIR, \"inference\")\n",
        "        return COCOEvaluator(dataset_name, cfg, True, output_folder)\n",
        "\n",
        "    def build_hooks(self):\n",
        "        hooks = super().build_hooks()\n",
        "        hooks.insert(-1,LossEvalHook(\n",
        "            cfg.TEST.EVAL_PERIOD_LOSS,\n",
        "            self.model,\n",
        "            build_detection_test_loader(\n",
        "                self.cfg,\n",
        "                self.cfg.DATASETS.TEST[0],\n",
        "                DatasetMapper(self.cfg,True)\n",
        "            )\n",
        "        ))\n",
        "        return hooks\n",
        "\n",
        "\n",
        "\n",
        "cfg = get_cfg()\n",
        "cfg.merge_from_file(model_zoo.get_config_file(\"COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml\"))\n",
        "cfg.OUTPUT_DIR = \"faster_rcnn_R_50_FPN_3x.yaml\"\n",
        "cfg.DATASETS.TRAIN = (\"dataset_train_1\",)\n",
        "cfg.DATASETS.TEST = (\"dataset_val_1\",)\n",
        "cfg.TEST.EVAL_PERIOD = 1000  # do a full coco evaluation every 500 iterations\n",
        "cfg.TEST.EVAL_PERIOD_LOSS = 500 # calculate the validation loss every 100 iterations\n",
        "cfg.DATALOADER.NUM_WORKERS = 2\n",
        "cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml\")  # Let training initialize from model zoo\n",
        "cfg.SOLVER.IMS_PER_BATCH = 4\n",
        "cfg.SOLVER.BASE_LR = 0.0001\n",
        "cfg.SOLVER.MAX_ITER = 5000\n",
        "cfg.SOLVER.STEPS = []\n",
        "cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 128\n",
        "cfg.MODEL.ROI_HEADS.NUM_CLASSES = 1\n",
        "cfg.MODEL.RPN.IOU_THRESHOLDS = [0.2, 0.4]\n",
        "\n",
        "\n",
        "\n",
        "# decrease anchor sizes:\n",
        "cfg.MODEL.ANCHOR_GENERATOR.SIZES = [[8], [14], [20], [26], [33]]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\n",
        "trainer = MyTrainer(cfg)\n",
        "trainer.resume_or_load(resume=False)\n",
        "trainer.train()\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}